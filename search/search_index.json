{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"ML.ENERGY  Research and Tech Blog","text":""},{"location":"energy/measurement/measuring-gpu-energy-best-practices/","title":"Measuring GPU Energy: Best Practices","text":"<p>To optimize something, you need to be able to measure it right. In this post, we'll look into potential pitfalls and best practices for GPU energy measurement.</p>"},{"location":"energy/measurement/measuring-gpu-energy-best-practices/#best-practices","title":"Best practices","text":""},{"location":"energy/measurement/measuring-gpu-energy-best-practices/#1-actually-measure-energy","title":"1. Actually measure energy","text":"<p>We sometimes see energy consumption or carbon emission being estimated with back-of-the-envelope calculations using the GPUs' Thermal Design Power (TDP), or in other words their maximum power consumption.<sup>1</sup> This is of course fine if you're trying to raise awareness and motivate others to start looking at energy consumption. However, if we want to really observe what happens to energy when we change around parameters and optimize it, we must actually measure energy consumption.</p> <p>TDP is usually not the best proxy for GPU power consumption. Below is the average power consumption of one NVIDIA V100 GPU while training different models:</p> <p> </p> GPU TDP is not the best proxy for average power consumption. <p>You can see that depending on the computation characteristic and load of models, average power consumption varies significantly, and never really touches the GPU's TDP.</p> <p>What about for larger models? Below we measured the average power consumption of four NVIDIA A40 GPUs training larger models with four-stage pipeline parallelism:</p> <p> </p> GPU TDP is not the best proxy for average power consumption. <p>It's the same again. Even for computation-heavy large model training, GPU average power consumption does not reach TDP.</p> <p>Takeaway</p> <p>GPU Thermal Design Power (TDP) is not the best estimate. Actually measure power and energy.</p>"},{"location":"energy/measurement/measuring-gpu-energy-best-practices/#2-use-the-most-efficient-api","title":"2. Use the most efficient API","text":"<p>How do you measure GPU energy? Depending on the microarchitecture of your GPU, there can be either one way or two ways. Below, we show the sample code using Python bindings for NVML.</p> Power API (All microarchitectures)Energy API (Volta or newer) <pre><code>import pynvml\n\npynvml.nvmlInit()\nhandle = pynvml.nvmlDeviceGetHandleByIndex(0)  # First GPU\npower = pynvml.nvmlDeviceGetPowerUsage(handle)  # milliwatts\n</code></pre> <p>The power API returns the current power consumption of the GPU. Since energy is power integrated over time, you will need a separate thread to poll the power API, and later integrate the power samples over time using <code>sklearn.metrics.auc</code>, for example.</p> <pre><code>import pynvml\n\npynvml.nvmlInit()\nhandle = pynvml.nvmlDeviceGetHandleByIndex(0)  # First GPU\nenergy = pynvml.nvmlDeviceGetTotalEnergyConsumption(handle)  # millijoules\n</code></pre> <p>The energy API returns the total energy consumption of the GPU since the driver was last loaded. Therefore, you just need to call the energy API once before computing and once after computing, and subtract the two for the energy consumption between the two calls.</p> <p>The power API, although supported by all microarchitectures, requires polling and then one discrete integration across time to compute energy. While polling happens in just one CPU core, it will be kept at high utilization during training and will consume some amount of extra energy purely for energy monitoring. On the other hand, the energy API simply requires two function calls in the main thread and one subtraction.</p> <p>Takeaway</p> <p>Use <code>nvmlDeviceGetTotalEnergyConsumption</code> if your GPU is Volta or newer. Otherwise, you'll need to poll <code>nvmlDeviceGetPowerUsage</code> and integrate power measurements across time to obtain energy.</p>"},{"location":"energy/measurement/measuring-gpu-energy-best-practices/#3-synchronize-cpu-and-gpu","title":"3. Synchronize CPU and GPU","text":"<p>In most DNN training frameworks, the CPU dispatches CUDA kernels to the GPU in an asynchronous fashion. In other words, the CPU tells the GPU to do some work and moves on to the next line of Python code without waiting for the GPU to complete. Consider this:</p> <pre><code>import time\nimport torch\n\nstart_time = time.time()\ntrain_one_step()\nelapsed_time = time.time() - start_time  # Wrong!\n</code></pre> <p>The time measured is likely an underestimation of GPU computation time, as the CPU code is running ahead of GPU code and the GPU may not have finished executing all the work ordered by the CPU. Therefore, there are APIs in PyTorch<sup>2</sup> and JAX<sup>3</sup> that synchronize CPU and GPU execution, i.e., have the CPU wait for the GPU is done:</p> <pre><code>import time\nimport torch\n\nstart_time = time.time()\ntrain_one_step()\ntorch.cuda.synchronize()  # Synchronizes CPU and GPU time.\nelapsed_time = time.time() - start_time\n</code></pre> <p>The same is the case for measuring GPU power or energy; NVML code that runs on your CPU must be synchronized with GPU execution for the measurement to be accurate:</p> <pre><code>import time\nimport torch\nimport pynvml\n\nstart_time = time.time()\nstart_energy = pynvml.nvmlDeviceGetTotalEnergyConsumption(handle)\ntrain_one_step()\ntorch.cuda.synchronize()  # Synchronizes CPU and GPU time.\nelapsed_time = time.time() - start_time\nconsumed_energy = \\\n    pynvml.nvmlDeviceGetTotalEnergyConsumption(handle) - start_energy\n</code></pre> <p>Takeaway</p> <p>To accurately measure GPU time and energy consumption, make the CPU wait for GPU work to complete. For example, in PyTorch, use <code>torch.cuda.synchronize</code>.</p>"},{"location":"energy/measurement/measuring-gpu-energy-best-practices/#the-all-in-one-solution-zeusmonitor","title":"The all-in-one solution: <code>ZeusMonitor</code>","text":"<p>Do all these feel like a headache? Well, <code>ZeusMonitor</code> got you covered. Simple. It implements all three best practices and provides a simple interface:</p> <pre><code>from zeus.monitor import ZeusMonitor\n\nmonitor = ZeusMonitor(gpu_indices=[0, 2])  # Arbitrary GPU indices.\n                                           # Respects `CUDA_VISIBLE_DEVICES`.\n\nmonitor.begin_window(\"training\")\n# Train!\nmeasurement = monitor.end_window(\"training\", sync_cuda=True)\n\nprint(f\"Time elapsed: {measurement.time}s\")\nprint(f\"GPU0 energy consumed: {measurement.energy[0]}J\")\nprint(f\"GPU2 energy consumed: {measurement.energy[2]}J\")\nprint(f\"Total energy consumed: {measurement.total_energy}J\")\n</code></pre> <p><code>ZeusMonitor</code> will automatically detect your GPU architecture (separately for each GPU index you with to monitor) and use the right NVML API to measure GPU time and energy consumption. You can have multiple overlapping measurement windows as long as you choose different names for them.</p> <p>Sounds good? Get started with Zeus here!</p> <ol> <li> <p>For instance, \"Estimating the Carbon Footprint of BLOOM, a 176B Parameter Language Model\" and \"Llama 2: Open Foundation and Fine-Tuned Chat Models\".\u00a0\u21a9</p> </li> <li> <p><code>torch.cuda.synchronize</code> \u21a9</p> </li> <li> <p><code>jax.block_until_ready</code> \u21a9</p> </li> </ol>"},{"location":"energy/research/ml-energy-performance-and-accuracy/","title":"ML Energy, Performance, and Accuracy","text":"<p>Zeus's batch size optimizer changes the model's training batch size to optimize time and energy consumption, but does that hurt the model's final quality? Short answer: No. Let's look into how in today's post.</p> <p>Zeus is the first energy measurement and optimization framework for Deep Learning. In our NSDI paper, we introduce an algorithm to automatically optimize a training job's time and energy consumption by finding the best the batch size and GPU power limit.</p>"},{"location":"energy/research/ml-energy-performance-and-accuracy/#two-categories-of-mlsys","title":"Two Categories of MLSys","text":"<p>Generally speaking, there are two big categories of systems that support machine learning:</p> <ol> <li>Training or inference quality (as measured by an appropriate ML metric like accuracy or perplexity) is the same compared to not using the system.</li> <li>It can be different (Usually worse quality, but the system is faster in some way or uses less memory).</li> </ol>"},{"location":"energy/research/ml-energy-performance-and-accuracy/#ensuring-model-quality","title":"Ensuring Model Quality","text":"<p>When you build systems for machine learning, one way to ensure that your system is of type 1 (no quality change) is to make sure it does the exact same computations that would have happened without the system. Data Parallel training on multiple GPUs is a good example. Even if you split the training batch across GPUs and compute the gradient separately, due to the arithmetic property of gradient computation, doing an AllReduce at the end recovers the gradient that would have been computed if you ran the entire batch on a hypothetical gigantic GPU.<sup>1</sup></p> <p>However, ensuring perfect computational equivalence is sometimes difficult and limits the scope of optimization. Thus, another emerging direction, is to allow system to do whatever in the end, but just make sure they reach the final target metric in the end. Time-To-Accuracy<sup>2</sup> is a very good example of this, which measures the wall clock time it took for a training system (regardless of whatever it does to the model) to reach the target validation accuracy of 93% on ImageNet. They have a similar metric for inference speed, too.</p>"},{"location":"energy/research/ml-energy-performance-and-accuracy/#which-category-is-zeus","title":"Which Category is Zeus?","text":"<p>Zeus is a type 1 MLSys because it minimizes a linear combination of Energy-To-Accuracy (ETA) and Time-To-Accuracy (TTA), where the user selects the target training validation metric. If the model does not reach the target metric, ETA and TTA are both infinity -- definitely not optimal.</p> <p>How is this optimization feasible? Changing the GPU's power limit does not change what is computed by the GPU, and hence there cannot be any model quality change. Changing the training batch size does change the computation done by the GPU (more importantly model convergence itself), but the room for optimization comes from the fact that there isn't just one batch size that can reach the target metric; there can be a couple, and Zeus will automatically find the best batch size among them.<sup>3</sup></p> <p>All in all, Zeus does not hurt model quality by design.</p>"},{"location":"energy/research/ml-energy-performance-and-accuracy/#what-about-performance","title":"What about Performance?","text":"<p>Performance, or more specifically training throughput, depends on the model and what you compare against. So there's a Pareto frontier of DNN training time and energy on GPUs. If the initial batch size and GPU power limit you've been using was not on the Pareto frontier, Zeus will reduce both training time and energy consumption by finding the optimal batch size and GPU power limit. However, if you happened to be training with the time-optimal configuration, but you still wanted to reduce energy consumption, Zeus will have to trade performance to reduce energy consumption. That is just how Pareto frontiers work -- On the frontier, there is no way to reduce one dimension without sacrificing the other dimensions.</p>"},{"location":"energy/research/ml-energy-performance-and-accuracy/#a-multi-objective-optimization-view","title":"A Multi-Objective Optimization View","text":"<p>When you want to optimize more than one competing objectives simultaneously, you have a Multi-Objective Optimization problem. As you can see, in general terms, we're facing a three-way multi-objective optimization problem, involving training time, model quality, and energy consumption.</p> <p>Zeus's approach was to (1) fix model quality (to what the user specified), and (2) perform linear scalarization for time and energy to make the optimization problem solvable. Speaking of which, for (2), why can't we use the \\(\\epsilon\\)-constraint method? Well, we can, and thus <code>GlobalPowerLimitOptimizer</code> supports both linear scalarization (<code>ZeusCost</code>) and \\(\\epsilon\\)-constraint (<code>MaxSlowdownConstraint</code>).</p> <ol> <li> <p>Well, to be precise, it won't be the exact same because on computers, the order of floating point operations can make a small difference in the final result even if the two order of operations are mathematically equivalent. Still, it's close enough.\u00a0\u21a9</p> </li> <li> <p>DAWNBench \u21a9</p> </li> <li> <p>Even in cases where there is really just one batch size that can reach the target metric, the user can disable batch size exploration by passing in a single set for the list of batch sizes of explore. Zeus still finds the optimal GPU power limit, leading to energy gains.\u00a0\u21a9</p> </li> </ol>"},{"location":"measurement/energy/llm-inference-energy-a-longitudinal-analysis/","title":"LLM Inference Energy: A Longitudinal Analysis","text":"<p>The ML.ENERGY Leaderboard went from v2.0 (September 2024) to v3.0 (December 2025) with major changes: up-to-date models, hardware, software, and datasets. The v3.0 blog post covered the details of the v3.0 results, but how to they compare to the times v2.0? Are we making progress on energy efficiency? In this short post, we would like to look at the impact of software optimizations on energy efficiency over time, using the Llama 3.1 family as a case study.</p>"},{"location":"measurement/energy/llm-inference-energy-a-longitudinal-analysis/#the-case-of-llama-31","title":"The Case of Llama 3.1","text":""},{"location":"measurement/energy/llm-inference-energy-a-longitudinal-analysis/#energy-reduction","title":"Energy Reduction","text":"<p>To reason about how the software stack affects energy, we fix the model and hardware: the Llama 3.1 family (8B, 70B, 405B) on H100 GPUs with the same number of GPUs.</p> <p> </p> Energy per output token vs. batch size for Llama 3.1 models on H100 GPUs. For 405B, V2 uses BF16 and V3 uses FP8. <p>The 8B and 70B models are exactly the same in V2 and V3. For the 405B model, we added one more layer: V3 uses native FP8 quantization.</p> <p>At small batch sizes, V3 uses substantially less energy per token than V2. For instance, the 8B model on batch size 64 reduces energy per token by 41% (0.20 J to 0.12 J). The 70B model shows consistent improvements across all batch sizes, with up to a 15% reduction. Similarly, the 405B model (especially with FP8) shows up to a 39% energy reduction at a batch size of 256. V2 or V3, the mathematical operations carried out by a small batch size are the same. Energy reductions come from software optimizations in vLLM (0.5.4 in V2 vs. 0.11.1 in V3). Notably, at small batch sizes where the latency of one iteration is low, CPU-side overheads may bottleneck execution and underutilize the GPU, increasing static power wastage and energy consumption. Software optimizations like vLLM Asynchronous Scheduling can mitigate this significantly.</p> <p>At larger batch sizes, the energy gap narrows. This is likely as batch size increases, the GPU becomes more fully utilized (with the same computations), and constant software-level loverheads are less significant relative to GPU computations.</p> <p>To understand things deeper, let's break down energy per token into its components:</p> \\[ \\frac{\\text{Energy}}{\\text{Token}} = \\frac{\\text{Power} \\cdot \\text{Time}}{\\text{Token}} = \\frac{\\text{Power}}{\\text{Token Throughput}} \\] <p>Let's look at each of these components.</p>"},{"location":"measurement/energy/llm-inference-energy-a-longitudinal-analysis/#token-throughput","title":"Token Throughput","text":"Token throughput vs. batch size for Llama 3.1 models on H100 GPUs. <p>At matched batch sizes, V3 achieves 3-5x higher throughput across all three model sizes. Progress in software optimizations (especially vLLM) over more than a year is the main driver of this improvement. V2's throughput curve is notably flat across batch sizes, suggesting the older vLLM version was unable to fully exploit increased parallelism from larger batches. V3 shows the expected pattern: throughput climbs steeply with batch size and eventually saturates as GPU compute and memory bandwidth become the bottleneck.</p> <p>With 3-5x throughput gains, one might expect proportional energy reductions. But as we saw, energy per token improved only 15-41%. Something is offsetting the throughput gains.</p>"},{"location":"measurement/energy/llm-inference-energy-a-longitudinal-analysis/#power","title":"Power","text":"Average GPU power vs. batch size for Llama 3.1 models on H100 GPUs. <p>vLLM GPU power draw is substantially higher in V3 than in V2. Power draw is a good indicator of the GPU's utilization as it reflect hardware circuit activity directly, which is the core factor that drove throughput improvements.<sup>1</sup></p>"},{"location":"measurement/energy/llm-inference-energy-a-longitudinal-analysis/#putting-it-together","title":"Putting It Together","text":"<p>V3's vLLM improvements keep the GPU busier, which both increases throughput (more tokens per second) and increases power draw (more of the GPU's circuitry is active). Since energy per token is the ratio of these two quantities, the energy improvement depends on which one grows faster. Throughput grew faster than power, resulting in a net energy reduction.</p> <p>Takeaway</p> <p>V3 achieves 3-5x higher throughput and 15-41% lower energy per token on Llama 3.1 models with the same hardware. Especially, at small batch sizes, we see nearly a halving of energy per token, which is a significant improvement for latency-sensitive applications. The reason energy per token improves less than throughput is that power also increases as the GPU is more fully utilized.</p>"},{"location":"measurement/energy/llm-inference-energy-a-longitudinal-analysis/#summing-up","title":"Summing Up","text":"<p>Energy consumption is a pressing issue across the AI stack, from individual inference requests to datacenter-scale deployments. But the V2-to-V3 transition shows that we are making relentless progress: software optimizations (vLLM 0.5.4 to 0.11.1) delivered 3-5x throughput improvements and measurable energy reductions.</p> <p>The small batch size (i.e., low latency) regime is the most challenging to run efficiently, and as we saw, that is also where software optimizations had the biggest impact on energy efficiency. This regime actually matters a lot for agentic applications that generate a ton of tokens; in order to keep latency in a reasonable range, these applications often run with small batch sizes. In this specific analysis, we see nearly a halving of energy per token at smaller batch sizes, which is a significant improvement for these applications.</p> <p>Progress spans the entire stack (inference engines, model architectures, quantization, and hardware) and the improvements compound. To understand how all these factors interact and to learn how to reason about and explain energy consumption, see our V3 leaderboard blog post.</p> <ol> <li> <p>Full disclosure: V2 only measured power during the entire benchmark duration (not explicitly during the steady-state window), which underestimates V2's power draw. This gets worse at larger batch sizes, where the relative duration of the steady state takes up a shorter portion of the entire duration. However, even with some underestimation of V2's power, the gap is large enough to conclude that vLLM draws more power in V3 than in V2.\u00a0\u21a9</p> </li> </ol>"},{"location":"measurement/energy/diagnosing-inference-energy-consumption-with-the-mlenergy-leaderboard-v30/","title":"Diagnosing Inference Energy Consumption with the ML.ENERGY Leaderboard v3.0","text":"<p>With The ML.ENERGY Benchmark v3.0 we released in December 2025, we expanded our scope to up-to-date important models, tasks, and GPU hardware. This included 46 models across 7 tasks, producing 1,858 configurations on NVIDIA H100 and B200 GPUs.<sup>1</sup> As always, latest benchmarking results are public and can be browsed on The ML.ENERGY Leaderboard.</p> <p>In this post, we first present empirical observations from measurements, and then develop a reasoning framework that explains why we observe certain energy behaviors.</p> <p>A PDF version of this post is available on arXiv. For more details on our benchmarking methodology, please refer to our NeurIPS 25 D&amp;B paper.</p>"},{"location":"measurement/energy/diagnosing-inference-energy-consumption-with-the-mlenergy-leaderboard-v30/#energy-by-architecture","title":"Energy by Architecture","text":"<p>What determines the energy consumption of generating one response? For Large Language Models (LLMs), a response is a complete answer to a prompt with all output tokens included. For diffusion models, a response is one generated image or video.</p>"},{"location":"measurement/energy/diagnosing-inference-energy-consumption-with-the-mlenergy-leaderboard-v30/#llm","title":"LLM","text":"<p>Task type heavily influences output length. LLM time and energy consumption is dominated by the decoding (token generation) phase. Different tasks naturally produce different distributions of output lengths. This is particularly pronounced between two LLM tasks in our benchmark: Problem Solving (reasoning on) and Text Conversation (reasoning off).</p> <p> </p> Distribution of (a) number of output tokens, (b) energy per token, and (c) energy per response across all models on B200 GPUs using their respective minimum-energy configurations. <p>Here, we're comparing the minimum-energy configuration<sup>2</sup> of each model on B200 GPUs, which allows us to focus on model and task differences without being confounded by hardware utilization differences. Problem Solving generates on average 10x more output tokens than Text Conversation (mean 6,988 vs. 717). Additionally, longer output sequences stress memory capacity and prevent larger batch sizes, increasing energy per token due to lower GPU utilization. Since energy per response is energy per token multiplied by the number of output tokens, these two factors multiply, resulting in Problem Solving consuming on average 25x more energy per response than Text Conversation (mean 4,625 J vs. 184 J).</p> <p>Case study on Qwen 3 32B on 1x B200. Qwen 3 32B supports both reasoning and non-reasoning, enabling direct comparison on the same model.</p> Metric Text Conversation Problem Solving Ratio Max batch size (BS) 512 128 0.25x Mean output tokens 627 7,035 11x Energy/token @ BS 128 0.209 J 0.312 J 1.5x Energy/token @ max BS 0.151 J 0.312 J 2.1x Energy/response 95 J 2,192 J 23x <p>Longer output sequences in Problem Solving increase the amount of KV cache memory usage per response, preventing larger batch sizes. Therefore, when we compare energy per token at each task's maximum batch size, Problem Solving is 2.1x higher. Even at the same batch size (128), longer sequences consume more energy per token due to higher memory footprint. Finally, combining longer outputs and higher energy per token results in 23x energy per response for Problem Solving for this model.</p> <p>Takeaway</p> <p>Task type heavily influences energy consumption. Notably, Problem Solving (reasoning) uses on average 25x more energy per response than Text Conversation. This comes from 10x more output tokens combined with higher energy per token due to memory pressure limiting batch size.</p>"},{"location":"measurement/energy/diagnosing-inference-energy-consumption-with-the-mlenergy-leaderboard-v30/#multimodal-llm","title":"Multimodal LLM","text":"<p>Multimodal LLMs (MLLMs) take images and/or videos alongside text as input and generate text responses.</p> <p> </p> (a) shows three models from the Qwen 3 family across three modalities (minimum-energy configurations), and (b) shows how modality affects batch size and KV cache utilization for the 8B model, showing why energy per token increases. Text modality uses the text-only model (e.g., Qwen 3 8B), whereas Image and Video use the vision-language variant (e.g., Qwen 3 VL 8B). <p>Multimodality can increase energy. The implications of multimodal inputs are threefold:</p> <ol> <li>Models run their modality encoder to convert inputs into multimodal tokens, which increases computation and memory operations and therefore energy consumption.</li> <li>In GPU memory-constrained scenarios, the modality encoder and the increase in input length increase memory usage, which can limit batch size.</li> <li>Multimodal inputs need to be preprocessed first on the CPU-side (e.g., converting raw image/video into tiles of pixels), which can take non-negligible time and become a bottleneck that further limits batch size.</li> </ol> <p>Indeed, when we compare minimum-energy configurations for different modalities, text + image inputs use 1.1-5.2x the energy per token of text, while text + video inputs use 1.3-15.0x.<sup>3</sup></p> <p>Case study on Qwen 3 (VL) 8B on 1x B200. We compare Qwen 3 8B on Text Conversation with Qwen 3 VL 8B on Image Chat and Video Chat tasks. For this smaller 8B model, the overheads of vision encoders and CPU-side preprocessing limit batch size significantly and underutilize the GPU. In particular, video inputs typically get converted to more vision tokens and are more expensive to preprocess on the CPU side, as shown by the much smaller batch size and higher energy per token. The drop in KV cache utilization as vision preprocessing overhead grows confirms that GPU memory was not the limiting factor---there was spare capacity for more tokens---but CPU-side vision preprocessing became a severe bottleneck that limited batch size.</p> <p>All in all, this is a case where GPU energy consumption is not just about the GPU; the entire system and the location of bottlenecks matter.<sup>4</sup> If CPU-side processing speed remains unchanged and only the GPU is upgraded, the GPU will only be more underutilized. In subsequent analyses, we do not include MLLMs because CPU-side bottlenecks make it difficult to isolate factors that impact GPU energy.</p> <p>Takeaway</p> <p>Multimodal inputs cost 1.1-5.2x (image) and 1.3-15.0x (video) the energy per token of text. CPU-side vision preprocessing can be a bottleneck that reduces batch size and increases energy per token.</p>"},{"location":"measurement/energy/diagnosing-inference-energy-consumption-with-the-mlenergy-leaderboard-v30/#diffusion-models","title":"Diffusion Models","text":"<p>We benchmarked diffusion models that generate images and videos from user text prompts. Diffusion is where model size is not the best predictor of energy consumption due to multiple runtime factors: number of inference (denoising) steps, output resolution, and number of frames (for video).</p> <p> </p> Energy per image/video for diffusion models (minimum-energy configuration on B200). SD is short for Stable Diffusion. <p>Text-to-image varies 20x across models. Models range from 0.6B to 12B parameters with 20-50 denoising steps. Notably, Hunyuan-DiT 1.2 (1.5B) consumes more energy than SD 3.5 Large (8.1B) despite fewer parameters, largely due to running 50 vs. 28 denoising steps.</p> <p>Text-to-video can be very energy intensive. Generating a single video consumes 26 kJ to 1.16 MJ---one to two orders of magnitude more than images. CogVideoX 1.5 5B uses more energy than Wan 2.1 14B despite being smaller, largely because it generates at higher resolution (768x1360 vs. 480x832). HunyuanVideo reaches 1.16 MJ because it generates 129 frames at 720p, resulting in 4x higher energy than Wan 2.1 14B (13B vs. 14B).</p> <p>We used default runtime parameters (denoising steps, resolution, frames) for all models. Many of these parameters are controllable by users, enabling navigation of the time-energy-quality tradeoff space. We explored this in a previous benchmark release.</p> <p>Takeaway</p> <p>Diffusion model energy depends on more than model size: number of denoising steps, output resolution, and frame count matter as much or more. Video generation can consume one to two orders of magnitude more energy than image generation.</p>"},{"location":"measurement/energy/diagnosing-inference-energy-consumption-with-the-mlenergy-leaderboard-v30/#deeper-dive-into-energy","title":"Deeper Dive into Energy","text":"<p>In this section, we measure and observe how different factors affect energy consumption.</p>"},{"location":"measurement/energy/diagnosing-inference-energy-consumption-with-the-mlenergy-leaderboard-v30/#batch-size","title":"Batch Size","text":"Energy per token, throughput, median ITL, and power trends against batch size for (a) DeepSeek R1 (Problem Solving) on 8x B200 and (b) Qwen 3 Coder 30B A3B (Code Completion) on 1x B200. Metrics normalized to % of maximum, except power which is normalized to % of GPU TDP. <p>The figure above shows the impact of batch size on energy per token, token generation throughput, median Inter-Token Latency (ITL), and GPU power draw. Computing hardware typically achieves peak energy efficiency when fully utilized (the Static Power Wastage section will go deeper into this). Therefore, as batch size increases, energy per token drops at first, then plateaus as GPU utilization approaches saturation.</p> <p>However, the energy efficiency gains of increasing batch size are not without tradeoffs. Latency (median ITL in this analysis) increases with batch size, as there is strictly more work to do for each batch. Throughput also increases with batch size, but with diminishing returns as GPU utilization reaches saturation. Finally, power draw increases with batch size, as a larger portion of the GPU's compute and memory circuitry is actively utilized and drawing power.</p> <p>From energy per token trends, we can see that DeepSeek R1 has not saturated GPU utilization even at the largest batch size that fits in memory, whereas Qwen 3 Coder approaches saturation around batch size 512. This explains the two models' throughput trends as well: DeepSeek R1 has a linearly increasing token throughput with batch size as GPU utilization keeps improving, whereas Qwen 3 Coder sees diminishing returns as it approaches saturation. We can see that these metrics move in tandem rather than in isolation, because they are all heavily coupled with latent factors like GPU utilization.</p> <p>Takeaway</p> <p>Increasing batch size increases latency, power, and throughput, but can unlock 3-5x energy per token reduction.</p>"},{"location":"measurement/energy/diagnosing-inference-energy-consumption-with-the-mlenergy-leaderboard-v30/#model-size-and-architecture","title":"Model Size and Architecture","text":"<p>With the Mixture-of-Experts (MoE) architecture, the number of active parameters is as important as the total number of parameters in energy consumption.</p> <p> </p> Energy/token by active parameters of Problem Solving models with the minimum-energy configuration on B200 GPUs. <p>The figure above compares models from the Qwen 3 family on the Problem Solving task using B200 GPUs: two MoE variants (30B A3B and 235B A22B) and three dense variants (8B, 14B, and 32B). For dense models, energy per token increases with the total number of parameters. However, when we include MoE models, we see that their energy per token is much lower than what a dense model of similar total number of parameters would consume. For instance, the energy per token of 30B A3B is 3.56x lower than that of 32B, despite having a similar total number of parameters. However, this is not to say that active parameters are now the only factor. 235B A22B consumes more energy than 32B as it needs to use more GPUs to fit all parameters in GPU memory, though it is still far less than what a dense 235B model would consume.</p> <p>Takeaway</p> <p>MoE models consume less energy compared to dense models of similar total number of parameters, making active parameters an important property for energy consumption. However, total parameters, which affect memory requirements, still play a role.</p>"},{"location":"measurement/energy/diagnosing-inference-energy-consumption-with-the-mlenergy-leaderboard-v30/#gpu-generation","title":"GPU Generation","text":"<p>One way to compare GPU models (B200 vs. H100) is to pick the minimum-energy configuration on each GPU at the same latency constraint.</p> <p> </p> B200 vs H100 energy comparison at latency constraints of 100 ms median ITL for LLMs and 30 s generation latency for Text to Image. Percentage of B200 energy reduction is annotated. <p>Energy reduction can vary significantly by model and task. Sometimes it is significantly better (e.g., 82% energy per token reduction for Qwen 3 235B A22B Thinking on Problem Solving), other times marginal or even worse, as we will see below.</p> <p>To get a better overall picture, we compare the two GPU models with three different latency constraints: 50/100/250 ms median ITL for LLMs, 10/30/60 s generation latency for Text to Image, 100/500/1000 s for Text to Video.</p> <p>LLM. Across all three median ITL constraints, B200 wins 88% (63/72) of comparisons with a median 35% energy reduction (ranging from 53% more to 82% less). A few notable exceptions happen at tight latency constraints. B200's large VRAM allows fitting large models on fewer GPUs, avoiding inter-GPU communication overhead. However, at tight latency constraints, using more H100 GPUs with a higher degree of parallelism can be more energy efficient. For example, at the 50 ms constraint, Qwen 3 30B A3B Thinking uses 53% less energy on 2x H100 (batch size 128) than on 1x B200 (batch size 64). Similarly, Qwen 3 235B A22B Instruct FP8 uses 33% less energy on 8x H100 (batch size 192) than on 2x B200 (batch size 64). At relaxed constraints (&gt; 50 ms), B200 wins as communication overhead is smaller and higher batch sizes become feasible. We will look deeper into multi-GPU scaling in the Multi-GPU Scaling section.</p> <p>Diffusion. For Text to Image, across all three latency constraints, B200 wins 86% (18/21) of comparisons with a median 15% energy reduction (ranging from 4% more to 23% less). Text to Video is also similar, with B200 winning 79% (11/14) of comparisons with a median 4% energy reduction (ranging from 6% more to 8% less). Cases where H100 wins (e.g., Stable Diffusion 3.5 Medium) are generally when the model is small enough to comfortably fit in one H100 GPU, meaning that it will underutilize a B200.</p> <p>We performed matched latency constraint comparisons, but we note that B200 would be capable of delivering lower latency than H100 when energy is not a concern due to its higher compute and memory throughput.</p> <p>Takeaway</p> <p>B200 achieves lower energy than H100 in 79-88% of comparisons at matched latency constraints. For tight LLM latency constraints, H100 can sometimes consume less energy by using more GPUs with higher parallelism to reduce latency. For Diffusion, B200 generally wins, unless the model is small.</p>"},{"location":"measurement/energy/diagnosing-inference-energy-consumption-with-the-mlenergy-leaderboard-v30/#precision","title":"Precision","text":"<p>FP8 quantization reduces model memory footprint and allows inference to leverage FP8 Tensor Cores with higher compute throughput. However, it also adds overhead from extra operations like input/activation quantization, dequantization, and scaling. We observe this tradeoff playing out differently at different batch sizes.</p> <p> </p> Qwen 3 235B A22B (Text Conversation) on 8x H100. FP8 loses at batch size 8-16, then wins at batch sizes from 32. The dashed vertical lines mark the crossover point. <p>FP8 wins at larger batch sizes. The figure above shows the energy per token and median ITL of Qwen 3 235B A22B (Text Conversation) on 8x H100 in both BF16 and FP8 across batch sizes. At smaller batch sizes, FP8 loses on both energy and latency due to (1) the overhead of extra operations (especially those that have not been fused into matrix multiplication), and (2) underutilization of the GPU, which prevents FP8 from leveraging its compute throughput advantage. If we compare FP8 and BF16 for all other models and tasks, we see a similar trend:</p> Energy Batch size FP8 wins Range Median 8-16 0/7 +13 to +56% +30% 17-64 6/13 -12 to +32% +1% 65-256 11/12 -29 to 0% -11% Latency Batch size FP8 wins Range Median 8-16 1/7 -5 to +26% +7% 17-64 11/13 -23 to +12% -12% 65-256 11/12 -18 to +3% -11% <p>At batch size 8-16, FP8 has higher energy (up to 56% more) and higher latency (up to 26% slower). As we grow batch size, we see FP8 starting to win on latency earlier, and then on energy as well. This is, at least in part, because GPUs are capable of delivering more theoretical FP8 compute throughput than BF16. Thus, at the same batch size, FP8 underutilizes the GPU more, leading to higher energy consumption until batch size is large enough to saturate the GPU.</p> <p>Qwen 3 Coder 480B A35B. This model is an exception; due to a limitation in vLLM at the time of benchmarking, the FP8 model had to run attention with data parallelism, while BF16 could run attention with tensor parallelism.<sup>5</sup> This made FP8 consistently consume more time and energy across all batch sizes. Attention data parallelism incurs load imbalance between GPUs that are assigned very different sequence lengths (e.g., some running long prefills whereas others run decode). Since the straggler GPU bottlenecks the entire batch, this can lead to significant latency overhead. Furthermore, the non-straggler GPUs do nothing and waste static power (see Static Power Wastage) waiting for the straggler, leading to even higher energy consumption as well.</p> <p>Takeaway</p> <p>At smaller batch sizes (8-16), FP8 can consume more time and/or energy than BF16. FP8 gains start to appear at larger batch sizes.</p>"},{"location":"measurement/energy/diagnosing-inference-energy-consumption-with-the-mlenergy-leaderboard-v30/#multi-gpu-scaling","title":"Multi-GPU Scaling","text":"<p>We can execute the same model on different numbers of GPUs, which affects both latency and energy consumption.</p> <p> </p> Time-energy tradeoffs of GPT-OSS 120B (Problem Solving). In both cases, scaling from 1 GPU to 2 GPUs at fixed batch size trades energy for time. In (b), 1 GPU is limited to batch size 64, while 2 GPUs unlock batch size 2,048 with less energy. <p>The figure above shows GPT OSS 120B on B200 and H100 with 1 and 2 GPUs. The plots are time-energy tradeoff curves, which are useful in comparing different configurations:</p> <ul> <li>The right-end of each curve represents the minimum-energy configuration for that GPU model and count.</li> <li>A vertical line at one's target latency finds minimum-energy configurations that meet the latency constraint.</li> <li>Jumping between curves following points with the same batch size shows the effect of GPU model and count.</li> </ul> <p>At the same batch size, more GPUs trade energy for latency. In general, increasing parallelism with more GPUs reduces latency but also increases energy at the same batch size because (1) latency does not decrease linearly due to communication overhead, and (2) less compute per GPU can lead to lower GPU utilization. Across B200 configurations, adding GPUs at the same batch size always increases energy per token and reduces latency in 81% of cases. Similarly, across H100 configurations, energy increases in 93% of the cases and latency always decreases.</p> <p>Memory capacity-bound cases unlock energy savings with more GPUs. On top of the above, in cases where adding more GPUs enables larger batch sizes due to increased aggregate memory capacity, we can see energy reductions. For GPT OSS 120B on 1x B200 with a 180 GB VRAM, the model already fits at high batch sizes on 1 GPU (batch size 3,072), so 2 GPUs only add overhead without enabling lower energy. On 1x H100 with an 80 GB VRAM, however, the server is limited to batch size 64, while 2 GPUs unlock batch size 2,048 and achieve 68% lower minimum energy. Thus, the model's total parameter memory footprint relative to the GPU's memory capacity is an important factor for whether multi-GPU scaling can reduce energy.</p> <p>Case study: Qwen 3 235B A22B Thinking FP8 on Problem Solving. As an extra case study, it is interesting to examine Qwen 3 235B A22B Thinking FP8 on Problem Solving with time-energy tradeoff frontiers for four sets of configurations (2x and 4x B200, 2x and 8x H100).</p> <p> </p> Time-energy tradeoff for Qwen 3 235B A22B Thinking FP8 on Problem Solving across B200 and H100 with different GPU counts. Each point is annotated with its batch size. <p>The 4x B200 curve (blue) Pareto-dominates, and also achieves the lowest possible energy (~0.4 J/token) by unlocking large batch sizes. 2x B200 (red) consumes less energy per token compared to 4x B200 (blue) at the same batch size at the cost of higher latency (as expected), and fails to scale to large batch sizes due to limited memory capacity. The two H100 configurations (purple and green) are right in the middle of the B200 curves; despite being a whole generation older, H100 is still competitive!</p> <p>Takeaway</p> <p>At the same batch size, more GPUs typically reduce latency but increase energy. When adding GPUs enables larger batch sizes, energy can be reduced, but only if serving was previously limited by memory capacity. H100 can still be competitive with B200 in terms of energy, especially when latency constraints are tight.</p>"},{"location":"measurement/energy/diagnosing-inference-energy-consumption-with-the-mlenergy-leaderboard-v30/#reasoning-about-energy-consumption","title":"Reasoning about Energy Consumption","text":"<p>In the previous sections, we presented empirical observations on energy consumption, but how can we act on them? In this section, we outline core mechanisms that govern energy consumption, with the goal of providing tools to explain and reason about energy consumption.</p>"},{"location":"measurement/energy/diagnosing-inference-energy-consumption-with-the-mlenergy-leaderboard-v30/#model-runtime-and-hardware-factors","title":"Model, Runtime, and Hardware Factors","text":"<p>Many factors across the whole system (hardware, software, and algorithm) affect energy consumption. Some of the key mechanisms are powerful but still straightforward. For instance, more computation generally means more energy consumption. As we have seen, diffusion models' energy increases with more denoising steps and higher output resolution (Diffusion Models), MoE models activate fewer parameters per token than dense models (Model Size and Architecture), and FP8 reduces circuit activity via lower-precision arithmetic (Precision). These are examples of choices at the runtime- and model-level directly affecting the amount of computation, and thus energy consumption.</p> <p>Another instance is hardware efficiency improvements over generations. Newer architectures typically deliver more operations per joule via various microarchitectural improvements and technology node shrinks. We have indeed seen that B200 generally consumes less energy than H100 (GPU Generation).</p>"},{"location":"measurement/energy/diagnosing-inference-energy-consumption-with-the-mlenergy-leaderboard-v30/#static-power-wastage","title":"Static Power Wastage","text":"<p>The power consumption of computing hardware, including GPUs, has two components: static power (consumed regardless of activity at all times) and dynamic power (reflects compute and memory activity). Let us consider a case where we executed some computation on a GPU, and only 60% of the GPU's compute units were utilized over the entire execution time. Here, the GPU will consume static power for the entire execution time, regardless of how well the GPU is utilized. Thus, 40% of the time the GPU is consuming static power while making little progress, effectively wasting energy. This is how low utilization increases static power wastage and thus energy consumption for the same amount of work.</p> <p>However, one of the most critical factors in GPU utilization is, in fact, not the GPU, but the rest of the system. That is, we want the GPU to be the sole bottleneck, not other system components. When CPU processing, network communication, disk I/O, or other parts of the system block GPU progress, the GPU does not have enough work to saturate itself or is even idle, wasting static power. Multimodal LLMs (Multimodal LLM) were a prime example: CPU-side vision preprocessing became a bottleneck that limited batch size, leaving the GPU underutilized despite having capacity for more concurrent requests. The result was higher energy per token---not because of the GPU, but because of the surrounding system.</p> <p>Another important factor is arithmetic intensity, i.e., the ratio of compute operations to the amount of memory movement. When arithmetic intensity is low, the GPU may be waiting on memory fetches more often than performing computations, leading to lower GPU utilization and higher static power wastage. We observed this for precision (Precision), where FP8 computations require extra operations that are not as arithmetically intensive as matrix multiplications. Thus, on smaller batch sizes, both FP8 extra operations and the smaller matrix multiplications had lower arithmetic intensity, leading to lower GPU utilization and offsetting savings from lower-precision arithmetic.</p> <p>This has interactions with earlier factors as well. For instance, upgrading to a newer hardware generation expecting better energy efficiency may not yield the expected benefits, or even worsen, if bottlenecks in the rest of the system were preventing the GPU from being fully utilized.</p>"},{"location":"measurement/energy/diagnosing-inference-energy-consumption-with-the-mlenergy-leaderboard-v30/#time-energy-tradeoff-frontier","title":"Time-Energy Tradeoff Frontier","text":"<p>There are many cases where there is a time-energy tradeoff frontier for the same amount of work. When the GPU ideally is the bottleneck, largely ruling out static power wastage (Static Power Wastage), we can navigate the time-energy tradeoff frontier through configuration choices.<sup>6</sup> In our analysis, the factors that govern this frontier are:</p> <ul> <li>Batch size: This is the primary knob that shapes and navigates the time-energy frontier.</li> <li>Memory capacity: Larger batches consume more memory. When GPU memory is saturated, we hit a ceiling, like we have seen for reasoning models in the LLM section. In other words, memory capacity bookends the frontier.</li> <li>Application constraints: Applications may come with latency deadlines or energy budgets. Larger batches increase per-request latency and reduce energy per work. Application-level latency and/or energy budgets allow us to select a point on the frontier.</li> </ul> <p>Batch size does not have to be the only knob that shapes the time-energy tradeoff frontier. For instance, the number of GPUs (Multi-GPU Scaling) can be effective, where adding GPUs increases aggregate memory capacity and also enables larger batch sizes that were not previously possible. While not explored in this post, GPU power limit and core frequency are also knobs that shape the frontier.</p>"},{"location":"measurement/energy/diagnosing-inference-energy-consumption-with-the-mlenergy-leaderboard-v30/#extending-to-ai-datacenters","title":"Extending to AI Datacenters","text":"<p>Our analysis so far has focused on energy consumption, but power is also an important metric to consider. Indeed, many AI datacenters today are power-constrained. Power availability caps the datacenter's power budget---either from the electricity grid (where drawing too much may not be approved or may cause reliability issues) or from on-site generation like natural gas and batteries (which take years to build).</p> <p>With power becoming the bottleneck resource, throughput per watt (e.g., tokens per second per watt, images per second per watt) is a critical metric for AI datacenter operators. For instance, tokens per second per watt can tell the operator how many average ChatGPT users the datacenter can serve within its power budget.</p> \\[ \\text{Throughput per Watt} = \\frac{\\text{Throughput}}{\\text{Power}} = \\frac{\\text{Work} / \\text{Time}}{\\text{Energy} / \\text{Time}} = \\frac{\\text{Work}}{\\text{Energy}} \\] <p>Throughput per watt is essentially the inverse of energy consumption per fixed work (e.g., energy per token, energy per image). Thus, optimizing energy consumption for the given work improves throughput per watt, closing the reasoning loop.</p> <p> </p> Energy and throughput/watt for four models on B200 with varying batch size. Note the log scale Y axis in (a). <p>As batch size increases, energy per token decreases and throughput per watt increases, both eventually plateauing as the GPU reaches saturation.</p>"},{"location":"measurement/energy/diagnosing-inference-energy-consumption-with-the-mlenergy-leaderboard-v30/#putting-everything-together","title":"Putting Everything Together","text":"A framework for reasoning about inference energy consumption in our analysis. Gray boxes are properties and knobs, blue boxes are latent factors, and orange boxes are the end metrics we observe from measurements and would like to understand and explain. <p>The figure above summarizes the structure of reasoning we have developed. Gray boxes are properties and low-level knobs of the algorithm, software, and hardware. Blue boxes represent latent variables that mediate between configurations and outcomes. Orange boxes show the end metrics we measure from benchmarks and ultimately want to understand.</p> <p>Causal structures like this show how different factors interact and propagate to affect the end metric and provide a framework for explaining empirical observations. When we observe unexpected energy behavior, we can trace through these factors to identify the root cause---whether it is memory constraints limiting batch size, CPU bottlenecks causing GPU underutilization, or compute volume increasing due to model choices. This also enables reasoning about optimization opportunities and hypothesizing about how they will affect energy consumption.</p> <p>Takeaway</p> <p>Energy consumption is governed by computation amount, hardware efficiency, GPU utilization, and the time-energy tradeoff frontier. System design choices---eliminating non-GPU bottlenecks and navigating the frontier via batch size and memory capacity---are key levers for optimization. Throughput per watt is the inverse of energy per output; optimizing one optimizes the other.</p>"},{"location":"measurement/energy/diagnosing-inference-energy-consumption-with-the-mlenergy-leaderboard-v30/#summing-up","title":"Summing Up","text":"<p>Each section already has key takeaways boxed.</p> <p>Things are more dynamic, complex, and nuanced than ever before in ML energy consumption. Without even thinking about energy, time itself is already so; throwing in energy clearly doesn't help!</p> <p>Therefore, we must measure. If you take away only one thing from this post, let it be that. Back-of-the-envelope estimates and rules of thumb only get you so far.</p> <p>And, it is very much possible to measure! We hope The ML.ENERGY Benchmark is a useful tool for you to measure, understand, and ultimately optimize energy consumption in your models, tasks, and systems. And we hope The ML.ENERGY Leaderboard provides useful reference points for numerous downstream use cases: model selection, system design, deployment planning, policymaking, and more.</p> <p>Finally, we welcome your feedback, questions, and suggestions. We're always looking forward to hearing from the community and collaborating. Find us at The ML.ENERGY Initiative homepage.</p> <ol> <li> <p>We used vLLM 0.11.1 for LLM/MLLMs and xDiT 0.4.5 for diffusion models.\u00a0\u21a9</p> </li> <li> <p>The configuration (e.g., batch size, number of GPUs) that achieves the lowest energy consumption for the model.\u00a0\u21a9</p> </li> <li> <p>One caveat of this cross-modality comparison is that, as we have seen in the LLM section, different tasks can have different output lengths that affect energy per token. For the models we compared, Text Conversation, Image Chat, and Video Chat have average output lengths of 808, 944, and 392 tokens, respectively. This isn't as large as the difference between Text Conversation and Problem Solving and shouldn't affect energy per token as much as that case, but Video Chat's shorter output length (which allows requests to finish faster and reduces batch size when the CPU is the bottleneck) may have increased energy per token compared to Image Chat and Text Conversation.\u00a0\u21a9</p> </li> <li> <p>There are some proposals to run vision preprocessing on the GPU itself (e.g., vLLM #21995), which can alleviate CPU-side bottlenecks but instead shift compute more to the GPU, which will likely introduce its own interesting tradeoffs.\u00a0\u21a9</p> </li> <li> <p>Standard parallelization methods used for LLMs: MoE models use expert parallelism with attention tensor parallelism; dense models use Tensor Parallelism for both MLP and attention. For Qwen 3 Coder 480B A35B FP8, see vLLM Recipes; last accessed 2024-12-26.\u00a0\u21a9</p> </li> <li> <p>When the GPU is being underutilized, a proper time-energy tradeoff frontier may not exist, as both time and energy can be reduced by improving GPU utilization.\u00a0\u21a9</p> </li> </ol>"},{"location":"energy/measurement/profiling-llm-energy-consumption-on-macs/","title":"Profiling LLM energy consumption on Macs","text":"<p>If you want to see how much energy LLM inference consumes on Apple Silicon, it's hard to find a straightforward way to do this programmatically, from within code. In this post, we'll explore how we can do this.</p> <p>The main tool available is a built-in macOS CLI utility called <code>powermetrics</code>, which prints energy metrics to output at set time intervals. However, it's annoying to use this programmatically (e.g., for precisely teasing apart energy consumption by different parts of code), because:</p> <ol> <li>It requires <code>sudo</code></li> <li>It measures and reports energy over set time intervals (e.g., 500 ms) instead of at arbitrary start/end points in your code</li> <li>You have to parse the output of the tool yourself in a background thread or process.</li> </ol> <p>So, to make this easier, we built zeus-apple-silicon, a very small C++/Python library designed specifically for energy profiling on Apple Silicon.</p> <p>Our hope with this library was to make something more straightforward to use -- it allows you to measure energy over any arbitrary block of code without needing periodic parsing in background threads. As a bonus, it provides more detailed readings than <code>powermetrics</code>. Whereas <code>powermetrics</code> only provides aggregate results for CPU, GPU, and ANE, this library gives you per-core energy (each efficiency/performance core separately), DRAM energy, and so on.</p> <p>The library is written/available in C++, but it\u2019s importable as a Python package via bindings. It can be installed with:</p> <pre><code>pip install zeus-apple-silicon\n</code></pre> <p>So, if you just want to know how much energy it takes to prompt a model, it can be as simple as:</p> <pre><code># Assumes `pip install llama-cpp-python huggingface-hub`\n\nfrom zeus_apple_silicon import AppleEnergyMonitor\nfrom llama_cpp import Llama\n\n# (1) Initialize your model\nllm = Llama.from_pretrained(\n    repo_id=\"bartowski/Llama-3.2-3B-Instruct-GGUF\",\n    filename=\"Llama-3.2-3B-Instruct-Q6_K.gguf\",\n    n_gpu_layers=-1,\n)\n\n# (2) Initialize energy monitor\nmonitor = AppleEnergyMonitor()\n\n# (3) See how much energy is consumed while generating response\nmonitor.begin_window(\"prompt\") # START an energy measurement window\noutput = llm.create_chat_completion(\n      messages = [\n          {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n          {\"role\": \"user\", \"content\": \"What makes a good Python library? Answer concisely.\"}\n      ],\n)\nenergy_metrics = monitor.end_window(\"prompt\") # END measurement, get results\n\nprint(\"--- Model Output ---\")\nprint(output[\"choices\"][0][\"message\"][\"content\"])\n\n# (4) Print energy usage over the measured window\nprint(\"--- Energy ---\")\nprint(energy_metrics)\n</code></pre> <p>And the output might look something like this:</p> <pre><code>--- Energy ---\nCPU Total: 25602 mJ\nEfficiency cores: 267 mJ  245 mJ\nPerformance cores: 5200 mJ  5268 mJ  4205 mJ  2678 mJ  1723 mJ  538 mJ  739 mJ  332 mJ\nEfficiency core manager: 301 mJ\nPerformance core manager: 4104 mJ\nDRAM: 16347 mJ\nGPU: 81962 mJ\nGPU SRAM: 4 mJ\nANE: 0 mJ\n</code></pre> <p>Note</p> <p>Some fields may be <code>None</code>, which happens when a processor doesn't support energy metrics for that field. On M1 chips, for instance, DRAM, ANE, and GPU SRAM results may not be available. On newer machines (M2 and above), all fields are typically present.</p> <p>Alternatively, if you\u2019re interfacing with low-level inference code directly (say, in llama.cpp), you can use the C++ version of the energy profiler, which is available as a header-only include, to tease apart energy metrics more precisely.</p> <p>For example, in a typical llama.cpp inference setup, you might repeatedly call <code>llama_decode</code> to run your model\u2019s forward pass over a batch of one or more tokens. So, you can wrap the <code>llama_decode</code> call in an energy profiling window, like this:</p> <pre><code>#include &lt;apple_energy.hpp&gt;\n\n/* ... Load model, initialize context, etc. */\n\nAppleEnergyMonitor monitor;\n\nwhile ( /* inference not finished */ ) {\n\n    // START energy measurement here\n    monitor.begin_window(\"batch\");\n\n    llama_decode(context, batch);\n\n    // END energy measurement here\n    AppleEnergyMetrics metrics = monitor.end_window(\"batch\");\n\n    // `metrics` contains energy consumed during call to `llama_decode`\n}\n</code></pre> <p>And this measurement would give you insight into the energy consumption per batch or token, ignoring one-time costs like model loading or context initialization.</p> <p>Tip</p> <p>You can obtain <code>apple_energy.hpp</code> from the zeus-apple-silicon GitHub repository.</p> <p>In terms of granularity, energy readings are updated basically as fast as the processor\u2019s energy counters are updated and passed through IOKit APIs, which is what the tool uses internally. When tested locally, updates were happening at less than 1 millisecond granularity.</p> <p>This library works as a standalone tool, but it was developed as part of a larger project called Zeus (GitHub: https://github.com/ml-energy/zeus), aimed at measuring/optimizing deep learning energy usage, particularly on GPUs. It offers the same window-based measurement API like <code>zeus-apple-silicon</code>, but supports broader hardware like NVIDIA and AMD GPUs, CPUs (mostly), DRAM (mostly), Apple Silicon, and NVIDIA Jetson platforms, and offers automated energy optimizers for Deep Learning scenarios alongside measurement.</p>"},{"location":"archive/2026/","title":"2026","text":""},{"location":"archive/2025/","title":"2025","text":""},{"location":"archive/2023/","title":"2023","text":""},{"location":"category/measurement/","title":"measurement","text":""},{"location":"category/energy/","title":"energy","text":""},{"location":"category/research/","title":"research","text":""}]}