{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"ML.ENERGY  Research and Tech Blog","text":""},{"location":"energy/measurement/measuring-gpu-energy-best-practices/","title":"Measuring GPU Energy: Best Practices","text":"<p>To optimize something, you need to be able to measure it right. In this post, we'll look into potential pitfalls and best practices for GPU energy measurement.</p>"},{"location":"energy/measurement/measuring-gpu-energy-best-practices/#best-practices","title":"Best practices","text":""},{"location":"energy/measurement/measuring-gpu-energy-best-practices/#1-actually-measure-energy","title":"1. Actually measure energy","text":"<p>We sometimes see energy consumption or carbon emission being estimated with back-of-the-envelope calculations using the GPUs' Thermal Design Power (TDP), or in other words their maximum power consumption.<sup>1</sup> This is of course fine if you're trying to raise awareness and motivate others to start looking at energy consumption. However, if we want to really observe what happens to energy when we change around parameters and optimize it, we must actually measure energy consumption.</p> <p>TDP is usually not the best proxy for GPU power consumption. Below is the average power consumption of one NVIDIA V100 GPU while training different models:</p> <p> </p> GPU TDP is not the best proxy for average power consumption. <p>You can see that depending on the computation characteristic and load of models, average power consumption varies significantly, and never really touches the GPU's TDP.</p> <p>What about for larger models? Below we measured the average power consumption of four NVIDIA A40 GPUs training larger models with four-stage pipeline parallelism:</p> <p> </p> GPU TDP is not the best proxy for average power consumption. <p>It's the same again. Even for computation-heavy large model training, GPU average power consumption does not reach TDP.</p> <p>Takeaway</p> <p>GPU Thermal Design Power (TDP) is not the best estimate. Actually measure power and energy.</p>"},{"location":"energy/measurement/measuring-gpu-energy-best-practices/#2-use-the-most-efficient-api","title":"2. Use the most efficient API","text":"<p>How do you measure GPU energy? Depending on the microarchitecture of your GPU, there can be either one way or two ways. Below, we show the sample code using Python bindings for NVML.</p> Power API (All microarchitectures)Energy API (Volta or newer) <pre><code>import pynvml\n\npynvml.nvmlInit()\nhandle = pynvml.nvmlDeviceGetHandleByIndex(0)  # First GPU\npower = pynvml.nvmlDeviceGetPowerUsage(handle)  # milliwatts\n</code></pre> <p>The power API returns the current power consumption of the GPU. Since energy is power integrated over time, you will need a separate thread to poll the power API, and later integrate the power samples over time using <code>sklearn.metrics.auc</code>, for example.</p> <pre><code>import pynvml\n\npynvml.nvmlInit()\nhandle = pynvml.nvmlDeviceGetHandleByIndex(0)  # First GPU\nenergy = pynvml.nvmlDeviceGetTotalEnergyConsumption(handle)  # millijoules\n</code></pre> <p>The energy API returns the total energy consumption of the GPU since the driver was last loaded. Therefore, you just need to call the energy API once before computing and once after computing, and subtract the two for the energy consumption between the two calls.</p> <p>The power API, although supported by all microarchitectures, requires polling and then one discrete integration across time to compute energy. While polling happens in just one CPU core, it will be kept at high utilization during training and will consume some amount of extra energy purely for energy monitoring. On the other hand, the energy API simply requires two function calls in the main thread and one subtraction.</p> <p>Takeaway</p> <p>Use <code>nvmlDeviceGetTotalEnergyConsumption</code> if your GPU is Volta or newer. Otherwise, you'll need to poll <code>nvmlDeviceGetPowerUsage</code> and integrate power measurements across time to obtain energy.</p>"},{"location":"energy/measurement/measuring-gpu-energy-best-practices/#3-synchronize-cpu-and-gpu","title":"3. Synchronize CPU and GPU","text":"<p>In most DNN training frameworks, the CPU dispatches CUDA kernels to the GPU in an asynchronous fashion. In other words, the CPU tells the GPU to do some work and moves on to the next line of Python code without waiting for the GPU to complete. Consider this:</p> <pre><code>import time\nimport torch\n\nstart_time = time.time()\ntrain_one_step()\nelapsed_time = time.time() - start_time  # Wrong!\n</code></pre> <p>The time measured is likely an underestimation of GPU computation time, as the CPU code is running ahead of GPU code and the GPU may not have finished executing all the work ordered by the CPU. Therefore, there are APIs in PyTorch<sup>2</sup> and JAX<sup>3</sup> that synchronize CPU and GPU execution, i.e., have the CPU wait for the GPU is done:</p> <pre><code>import time\nimport torch\n\nstart_time = time.time()\ntrain_one_step()\ntorch.cuda.synchronize()  # Synchronizes CPU and GPU time.\nelapsed_time = time.time() - start_time\n</code></pre> <p>The same is the case for measuring GPU power or energy; NVML code that runs on your CPU must be synchronized with GPU execution for the measurement to be accurate:</p> <pre><code>import time\nimport torch\nimport pynvml\n\nstart_time = time.time()\nstart_energy = pynvml.nvmlDeviceGetTotalEnergyConsumption(handle)\ntrain_one_step()\ntorch.cuda.synchronize()  # Synchronizes CPU and GPU time.\nelapsed_time = time.time() - start_time\nconsumed_energy = \\\n    pynvml.nvmlDeviceGetTotalEnergyConsumption(handle) - start_energy\n</code></pre> <p>Takeaway</p> <p>To accurately measure GPU time and energy consumption, make the CPU wait for GPU work to complete. For example, in PyTorch, use <code>torch.cuda.synchronize</code>.</p>"},{"location":"energy/measurement/measuring-gpu-energy-best-practices/#the-all-in-one-solution-zeusmonitor","title":"The all-in-one solution: <code>ZeusMonitor</code>","text":"<p>Do all these feel like a headache? Well, <code>ZeusMonitor</code> got you covered. Simple. It implements all three best practices and provides a simple interface:</p> <pre><code>from zeus.monitor import ZeusMonitor\n\nmonitor = ZeusMonitor(gpu_indices=[0, 2])  # Arbitrary GPU indices.\n                                           # Respects `CUDA_VISIBLE_DEVICES`.\n\nmonitor.begin_window(\"training\")\n# Train!\nmeasurement = monitor.end_window(\"training\", sync_cuda=True)\n\nprint(f\"Time elapsed: {measurement.time}s\")\nprint(f\"GPU0 energy consumed: {measurement.energy[0]}J\")\nprint(f\"GPU2 energy consumed: {measurement.energy[2]}J\")\nprint(f\"Total energy consumed: {measurement.total_energy}J\")\n</code></pre> <p><code>ZeusMonitor</code> will automatically detect your GPU architecture (separately for each GPU index you with to monitor) and use the right NVML API to measure GPU time and energy consumption. You can have multiple overlapping measurement windows as long as you choose different names for them.</p> <p>Sounds good? Get started with Zeus here!</p> <ol> <li> <p>For instance, \"Estimating the Carbon Footprint of BLOOM, a 176B Parameter Language Model\" and \"Llama 2: Open Foundation and Fine-Tuned Chat Models\".\u00a0\u21a9</p> </li> <li> <p><code>torch.cuda.synchronize</code> \u21a9</p> </li> <li> <p><code>jax.block_until_ready</code> \u21a9</p> </li> </ol>"},{"location":"energy/research/ml-energy-performance-and-accuracy/","title":"ML Energy, Performance, and Accuracy","text":"<p>Zeus's batch size optimizer changes the model's training batch size to optimize time and energy consumption, but does that hurt the model's final quality? Short answer: No. Let's look into how in today's post.</p> <p>Zeus is the first energy measurement and optimization framework for Deep Learning. In our NSDI paper, we introduce an algorithm to automatically optimize a training job's time and energy consumption by finding the best the batch size and GPU power limit.</p>"},{"location":"energy/research/ml-energy-performance-and-accuracy/#two-categories-of-mlsys","title":"Two Categories of MLSys","text":"<p>Generally speaking, there are two big categories of systems that support machine learning:</p> <ol> <li>Training or inference quality (as measured by an appropriate ML metric like accuracy or perplexity) is the same compared to not using the system.</li> <li>It can be different (Usually worse quality, but the system is faster in some way or uses less memory).</li> </ol>"},{"location":"energy/research/ml-energy-performance-and-accuracy/#ensuring-model-quality","title":"Ensuring Model Quality","text":"<p>When you build systems for machine learning, one way to ensure that your system is of type 1 (no quality change) is to make sure it does the exact same computations that would have happened without the system. Data Parallel training on multiple GPUs is a good example. Even if you split the training batch across GPUs and compute the gradient separately, due to the arithmetic property of gradient computation, doing an AllReduce at the end recovers the gradient that would have been computed if you ran the entire batch on a hypothetical gigantic GPU.<sup>1</sup></p> <p>However, ensuring perfect computational equivalence is sometimes difficult and limits the scope of optimization. Thus, another emerging direction, is to allow system to do whatever in the end, but just make sure they reach the final target metric in the end. Time-To-Accuracy<sup>2</sup> is a very good example of this, which measures the wall clock time it took for a training system (regardless of whatever it does to the model) to reach the target validation accuracy of 93% on ImageNet. They have a similar metric for inference speed, too.</p>"},{"location":"energy/research/ml-energy-performance-and-accuracy/#which-category-is-zeus","title":"Which Category is Zeus?","text":"<p>Zeus is a type 1 MLSys because it minimizes a linear combination of Energy-To-Accuracy (ETA) and Time-To-Accuracy (TTA), where the user selects the target training validation metric. If the model does not reach the target metric, ETA and TTA are both infinity -- definitely not optimal.</p> <p>How is this optimization feasible? Changing the GPU's power limit does not change what is computed by the GPU, and hence there cannot be any model quality change. Changing the training batch size does change the computation done by the GPU (more importantly model convergence itself), but the room for optimization comes from the fact that there isn't just one batch size that can reach the target metric; there can be a couple, and Zeus will automatically find the best batch size among them.<sup>3</sup></p> <p>All in all, Zeus does not hurt model quality by design.</p>"},{"location":"energy/research/ml-energy-performance-and-accuracy/#what-about-performance","title":"What about Performance?","text":"<p>Performance, or more specifically training throughput, depends on the model and what you compare against. So there's a Pareto frontier of DNN training time and energy on GPUs. If the initial batch size and GPU power limit you've been using was not on the Pareto frontier, Zeus will reduce both training time and energy consumption by finding the optimal batch size and GPU power limit. However, if you happened to be training with the time-optimal configuration, but you still wanted to reduce energy consumption, Zeus will have to trade performance to reduce energy consumption. That is just how Pareto frontiers work -- On the frontier, there is no way to reduce one dimension without sacrificing the other dimensions.</p>"},{"location":"energy/research/ml-energy-performance-and-accuracy/#a-multi-objective-optimization-view","title":"A Multi-Objective Optimization View","text":"<p>When you want to optimize more than one competing objectives simultaneously, you have a Multi-Objective Optimization problem. As you can see, in general terms, we're facing a three-way multi-objective optimization problem, involving training time, model quality, and energy consumption.</p> <p>Zeus's approach was to (1) fix model quality (to what the user specified), and (2) perform linear scalarization for time and energy to make the optimization problem solvable. Speaking of which, for (2), why can't we use the \\(\\epsilon\\)-constraint method? Well, we can, and thus <code>GlobalPowerLimitOptimizer</code> supports both linear scalarization (<code>ZeusCost</code>) and \\(\\epsilon\\)-constraint (<code>MaxSlowdownConstraint</code>).</p> <ol> <li> <p>Well, to be precise, it won't be the exact same because on computers, the order of floating point operations can make a small difference in the final result even if the two order of operations are mathematically equivalent. Still, it's close enough.\u00a0\u21a9</p> </li> <li> <p>DAWNBench \u21a9</p> </li> <li> <p>Even in cases where there is really just one batch size that can reach the target metric, the user can disable batch size exploration by passing in a single set for the list of batch sizes of explore. Zeus still finds the optimal GPU power limit, leading to energy gains.\u00a0\u21a9</p> </li> </ol>"},{"location":"energy/measurement/profiling-llm-energy-consumption-on-macs/","title":"Profiling LLM energy consumption on Macs","text":"<p>If you want to see how much energy LLM inference consumes on Apple Silicon, it's hard to find a straightforward way to do this programmatically, from within code. In this post, we'll explore how we can do this.</p> <p>The main tool available is a built-in macOS CLI utility called <code>powermetrics</code>, which prints energy metrics to output at set time intervals. However, it's annoying to use this programmatically (e.g., for precisely teasing apart energy consumption by different parts of code), because:</p> <ol> <li>It requires <code>sudo</code></li> <li>It measures and reports energy over set time intervals (e.g., 500 ms) instead of at arbitrary start/end points in your code</li> <li>You have to parse the output of the tool yourself in a background thread or process.</li> </ol> <p>So, to make this easier, we built zeus-apple-silicon, a very small C++/Python library designed specifically for energy profiling on Apple Silicon.</p> <p>Our hope with this library was to make something more straightforward to use -- it allows you to measure energy over any arbitrary block of code without needing periodic parsing in background threads. As a bonus, it provides more detailed readings than <code>powermetrics</code>. Whereas <code>powermetrics</code> only provides aggregate results for CPU, GPU, and ANE, this library gives you per-core energy (each efficiency/performance core separately), DRAM energy, and so on.</p> <p>The library is written/available in C++, but it\u2019s importable as a Python package via bindings. It can be installed with:</p> <pre><code>pip install zeus-apple-silicon\n</code></pre> <p>So, if you just want to know how much energy it takes to prompt a model, it can be as simple as:</p> <pre><code># Assumes `pip install llama-cpp-python huggingface-hub`\n\nfrom zeus_apple_silicon import AppleEnergyMonitor\nfrom llama_cpp import Llama\n\n# (1) Initialize your model\nllm = Llama.from_pretrained(\n    repo_id=\"bartowski/Llama-3.2-3B-Instruct-GGUF\",\n    filename=\"Llama-3.2-3B-Instruct-Q6_K.gguf\",\n    n_gpu_layers=-1,\n)\n\n# (2) Initialize energy monitor\nmonitor = AppleEnergyMonitor()\n\n# (3) See how much energy is consumed while generating response\nmonitor.begin_window(\"prompt\") # START an energy measurement window\noutput = llm.create_chat_completion(\n      messages = [\n          {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n          {\"role\": \"user\", \"content\": \"What makes a good Python library? Answer concisely.\"}\n      ],\n)\nenergy_metrics = monitor.end_window(\"prompt\") # END measurement, get results\n\nprint(\"--- Model Output ---\")\nprint(output[\"choices\"][0][\"message\"][\"content\"])\n\n# (4) Print energy usage over the measured window\nprint(\"--- Energy ---\")\nprint(energy_metrics)\n</code></pre> <p>And the output might look something like this:</p> <pre><code>--- Energy ---\nCPU Total: 25602 mJ\nEfficiency cores: 267 mJ  245 mJ\nPerformance cores: 5200 mJ  5268 mJ  4205 mJ  2678 mJ  1723 mJ  538 mJ  739 mJ  332 mJ\nEfficiency core manager: 301 mJ\nPerformance core manager: 4104 mJ\nDRAM: 16347 mJ\nGPU: 81962 mJ\nGPU SRAM: 4 mJ\nANE: 0 mJ\n</code></pre> <p>Note</p> <p>Some fields may be <code>None</code>, which happens when a processor doesn't support energy metrics for that field. On M1 chips, for instance, DRAM, ANE, and GPU SRAM results may not be available. On newer machines (M2 and above), all fields are typically present.</p> <p>Alternatively, if you\u2019re interfacing with low-level inference code directly (say, in llama.cpp), you can use the C++ version of the energy profiler, which is available as a header-only include, to tease apart energy metrics more precisely.</p> <p>For example, in a typical llama.cpp inference setup, you might repeatedly call <code>llama_decode</code> to run your model\u2019s forward pass over a batch of one or more tokens. So, you can wrap the <code>llama_decode</code> call in an energy profiling window, like this:</p> <pre><code>#include &lt;apple_energy.hpp&gt;\n\n/* ... Load model, initialize context, etc. */\n\nAppleEnergyMonitor monitor;\n\nwhile ( /* inference not finished */ ) {\n\n    // START energy measurement here\n    monitor.begin_window(\"batch\");\n\n    llama_decode(context, batch);\n\n    // END energy measurement here\n    AppleEnergyMetrics metrics = monitor.end_window(\"batch\");\n\n    // `metrics` contains energy consumed during call to `llama_decode`\n}\n</code></pre> <p>And this measurement would give you insight into the energy consumption per batch or token, ignoring one-time costs like model loading or context initialization.</p> <p>Tip</p> <p>You can obtain <code>apple_energy.hpp</code> from the zeus-apple-silicon GitHub repository.</p> <p>In terms of granularity, energy readings are updated basically as fast as the processor\u2019s energy counters are updated and passed through IOKit APIs, which is what the tool uses internally. When tested locally, updates were happening at less than 1 millisecond granularity.</p> <p>This library works as a standalone tool, but it was developed as part of a larger project called Zeus (GitHub: https://github.com/ml-energy/zeus), aimed at measuring/optimizing deep learning energy usage, particularly on GPUs. It offers the same window-based measurement API like <code>zeus-apple-silicon</code>, but supports broader hardware like NVIDIA and AMD GPUs, CPUs (mostly), DRAM (mostly), Apple Silicon, and NVIDIA Jetson platforms, and offers automated energy optimizers for Deep Learning scenarios alongside measurement.</p>"},{"location":"archive/2025/","title":"2025","text":""},{"location":"archive/2023/","title":"2023","text":""},{"location":"category/energy/","title":"energy","text":""},{"location":"category/measurement/","title":"measurement","text":""},{"location":"category/research/","title":"research","text":""}]}