{"version": "https://jsonfeed.org/version/1.1", "title": "ML.ENERGY Blog", "home_page_url": "https://ml.energy/blog/", "feed_url": "https://ml.energy/blog/feed_json_updated.json", "description": "ML.ENERGY research & tech blog", "icon": null, "authors": [{"name": "The ML.ENERGY Initiative"}], "language": "en", "items": [{"id": "https://ml.energy/blog/energy/measurement/measuring-gpu-energy-best-practices/", "url": "https://ml.energy/blog/energy/measurement/measuring-gpu-energy-best-practices/", "title": "Measuring GPU Energy: Best Practices", "content_html": "<h1>Measuring GPU Energy: Best Practices</h1>\n<p>To optimize something, you need to be able to measure it right.\nIn this post, we'll look into potential pitfalls and best practices for GPU energy measurement.</p>", "image": "https://ml.energy/blog/assets/img/social/energy/measurement/measuring-gpu-energy-best-practices.png", "date_modified": "2026-02-18T18:01:46+00:00", "date_published": "2023-07-24T00:00:00+00:00", "authors": [{"name": "Jae-Won Chung"}], "tags": ["energy", "measurement"]}, {"id": "https://ml.energy/blog/energy/research/ml-energy-performance-and-accuracy/", "url": "https://ml.energy/blog/energy/research/ml-energy-performance-and-accuracy/", "title": "ML Energy, Performance, and Accuracy", "content_html": "<h1>ML Energy, Performance, and Accuracy</h1>\n<p>Zeus's <a href=\"https://ml.energy/zeus/optimize/batch_size_optimizer\">batch size optimizer</a> changes the model's training batch size to optimize time and energy consumption, but does that hurt the model's final quality?\nShort answer: No.\nLet's look into how in today's post.</p>", "image": "https://ml.energy/blog/assets/img/social/energy/research/ml-energy-performance-and-accuracy.png", "date_modified": "2026-02-18T18:01:46+00:00", "date_published": "2023-08-01T00:00:00+00:00", "authors": [{"name": "Jae-Won Chung"}], "tags": ["energy", "research"]}, {"id": "https://ml.energy/blog/measurement/energy/llm-inference-energy-a-longitudinal-analysis/", "url": "https://ml.energy/blog/measurement/energy/llm-inference-energy-a-longitudinal-analysis/", "title": "LLM Inference Energy: A Longitudinal Analysis", "content_html": "<h1>LLM Inference Energy: A Longitudinal Analysis</h1>\n<p>The ML.ENERGY Leaderboard went from v2.0 (September 2024) to <a href=\"https://ml.energy/leaderboard\">v3.0</a> (December 2025) with major changes: up-to-date models, hardware, software, and datasets.\nThe <a href=\"ml-energy-leaderboard-v3.0.md\">v3.0 blog post</a> covered the details of the v3.0 results, but how to they compare to the times v2.0?\nAre we making progress on energy efficiency?\nIn this short post, we would like to look at the impact of <strong>software optimizations</strong> on energy efficiency over time, using the Llama 3.1 family as a case study.</p>", "image": "https://ml.energy/blog/assets/img/social/measurement/energy/llm-inference-energy-a-longitudinal-analysis.png", "date_modified": "2026-02-18T18:01:46+00:00", "date_published": "2026-02-09T00:00:00+00:00", "authors": [{"name": "Jae-Won Chung"}], "tags": ["energy", "measurement"]}, {"id": "https://ml.energy/blog/measurement/energy/diagnosing-inference-energy-consumption-with-the-mlenergy-leaderboard-v30/", "url": "https://ml.energy/blog/measurement/energy/diagnosing-inference-energy-consumption-with-the-mlenergy-leaderboard-v30/", "title": "Diagnosing Inference Energy Consumption with the ML.ENERGY Leaderboard v3.0", "content_html": "<h1>Diagnosing Inference Energy Consumption with the ML.ENERGY Leaderboard v3.0</h1>\n<p>With <a href=\"https://github.com/ml-energy/benchmark/releases/tag/v3.0\">The ML.ENERGY Benchmark v3.0</a> we released in December 2025, we expanded our scope to up-to-date important models, tasks, and GPU hardware.\nThis included 46 models across 7 tasks, producing 1,858 configurations on NVIDIA H100 and B200 GPUs.[^software-setup]\nAs always, latest benchmarking results are public and can be browsed on <a href=\"https://ml.energy/leaderboard\">The ML.ENERGY Leaderboard</a>.</p>\n<p>In this post, we first present empirical observations from measurements, and then develop a reasoning framework that explains <em>why</em> we observe certain energy behaviors.</p>", "image": "https://ml.energy/blog/assets/img/social/measurement/energy/diagnosing-inference-energy-consumption-with-the-mlenergy-leaderboard-v30.png", "date_modified": "2026-02-18T18:01:46+00:00", "date_published": "2026-01-29T00:00:00+00:00", "authors": [{"name": "Jae-Won Chung"}], "tags": ["energy", "measurement"]}, {"id": "https://ml.energy/blog/energy/measurement/profiling-llm-energy-consumption-on-macs/", "url": "https://ml.energy/blog/energy/measurement/profiling-llm-energy-consumption-on-macs/", "title": "Profiling LLM energy consumption on Macs", "content_html": "<h1>Profiling LLM energy consumption on Macs</h1>\n<p>If you want to see how much energy LLM inference consumes on Apple Silicon, it's hard to find a straightforward way to do this programmatically, from within code. In this post, we'll explore how we can do this.</p>", "image": "https://ml.energy/blog/assets/img/social/energy/measurement/profiling-llm-energy-consumption-on-macs.png", "date_modified": "2026-02-18T18:01:46+00:00", "date_published": "2025-05-17T00:00:00+00:00", "authors": [{"name": "Jisang (Michael) Ahn"}, {"name": "Jae-Won Chung"}], "tags": ["energy", "measurement"]}]}